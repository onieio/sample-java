from pyspark import SparkContext
from pyspark.sql import SQLContext

if __name__ == "__main__":
    """
        Usage: spark-submit [options]
    """

    sc = SparkContext(appName="Pyspark_First")

    sql_context = SQLContext(sc)
    redshift_url = "jdbc:redshift://prod-tracking-redshift.xxxxxxx.ap-south-1.redshift.amazonaws.com:5439/dbname?user=user_name&password=user_passwd"
    #redshift_query  = "select query limit 10"
    redshift_query_tempdir_storage = "s3://bucket/folder"
    df_users = sql_context.read \
        .format("com.databricks.spark.redshift") \
        .option("url", redshift_url) \
        .option("query", redshift_query) \
        .option("tempdir", redshift_query_tempdir_storage) \
        .option("forward_spark_s3_credentials", "true") \
        .load()
    df_users.write \
        .format("com.databricks.spark.redshift") \
        .option("url", redshift_url) \
        .option("dbtable", "dummy_table_one") \
        .option("tempdir", redshift_query_tempdir_storage) \
        .option("aws_iam_role","arn:aws:iam::949397323834:role/redshift-s3-spark") \
        .mode("error") \
        .save()
    print("value", df_users.count(), type(df_users))
    print("value", df_users.show())
    print("Devops")

sc.stop()